{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mikel/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/tensorpack/callbacks/hooks.py:17: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mikel/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/tensorpack/tfutils/optimizer.py:18: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mikel/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/tensorpack/tfutils/sesscreate.py:20: The name tf.train.SessionCreator is deprecated. Please use tf.compat.v1.train.SessionCreator instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import json\n",
    "import torch\n",
    "from types import SimpleNamespace\n",
    "from vilbert.vilbert import VILBertActionGrounding, BertConfig\n",
    "from pytorch_transformers.tokenization_bert import BertTokenizer\n",
    "from pytorch_transformers.optimization import AdamW, WarmupLinearSchedule\n",
    "import torch.distributed as dist\n",
    "from VLN_config import config as args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import argparse\n",
    "import glob\n",
    "import pdb\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from faster_rcnn import feature_extractor_new as f_extractor\n",
    "from faster_rcnn.feature_extractor_new import featureExtractor\n",
    "#%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(adam_epsilon=1e-08, baseline=False, bert_model='bert-base-uncased', best_features=10, clean_train_sets=True, config_file='config/bert_base_6layer_6conect.json', distributed=False, do_lower_case=True, dynamic_attention=False, from_pretrained='save/multitask_model/multi_task_model.bin', gradient_accumulation_steps=1, img_weight=1, in_memory=False, learning_rate=0.0001, local_rank=-1, max_temporal_memory_buffer=3, mean_layer=True, num_train_epochs=10.0, num_workers=0, objective=1, predict_feature=False, save_name='', seed=42, split='mteval', start_epoch=0, task_specific_tokens=True, tasks='1', threshold_similarity=0.7, track_temporal_features=True, train_batch_size=1, visual_target=0, warmup_proportion=0.1, without_coattention=False)\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/20/2020 18:41:01 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/mikel/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "07/20/2020 18:41:01 - INFO - vilbert.utils -   loading weights file save/multitask_model/multi_task_model.bin\n",
      "07/20/2020 18:41:07 - INFO - vilbert.utils -   Weights of VILBertActionGrounding not initialized from pretrained model: ['positional_enc.weight', 'positional_enc.bias', 'img_emb_mean.weight', 'img_emb_mean.bias', 'action_cls.predictions.bias', 'action_cls.predictions.transform.dense.weight', 'action_cls.predictions.transform.dense.bias', 'action_cls.predictions.transform.LayerNorm.weight', 'action_cls.predictions.transform.LayerNorm.bias', 'action_cls.predictions.decoder.weight', 'action_cls.bi_seq_relationship.weight', 'action_cls.bi_seq_relationship.bias', 'action_cls.imagePredictions.transform.dense.weight', 'action_cls.imagePredictions.transform.dense.bias', 'action_cls.imagePredictions.transform.LayerNorm.weight', 'action_cls.imagePredictions.transform.LayerNorm.bias', 'action_cls.imagePredictions.decoder.weight', 'action_cls.imagePredictions.decoder.bias']\n",
      "07/20/2020 18:41:07 - INFO - vilbert.utils -   Weights from pretrained model not used in VILBertActionGrounding: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.bi_seq_relationship.weight', 'cls.bi_seq_relationship.bias', 'cls.imagePredictions.transform.dense.weight', 'cls.imagePredictions.transform.dense.bias', 'cls.imagePredictions.transform.LayerNorm.weight', 'cls.imagePredictions.transform.LayerNorm.bias', 'cls.imagePredictions.decoder.weight', 'cls.imagePredictions.decoder.bias', 'vil_prediction.logit_fc.0.weight', 'vil_prediction.logit_fc.0.bias', 'vil_prediction.logit_fc.2.weight', 'vil_prediction.logit_fc.2.bias', 'vil_prediction.logit_fc.3.weight', 'vil_prediction.logit_fc.3.bias', 'vil_prediction_gqa.logit_fc.0.weight', 'vil_prediction_gqa.logit_fc.0.bias', 'vil_prediction_gqa.logit_fc.2.weight', 'vil_prediction_gqa.logit_fc.2.bias', 'vil_prediction_gqa.logit_fc.3.weight', 'vil_prediction_gqa.logit_fc.3.bias', 'vil_binary_prediction.logit_fc.0.weight', 'vil_binary_prediction.logit_fc.0.bias', 'vil_binary_prediction.logit_fc.2.weight', 'vil_binary_prediction.logit_fc.2.bias', 'vil_binary_prediction.logit_fc.3.weight', 'vil_binary_prediction.logit_fc.3.bias', 'vil_logit.weight', 'vil_logit.bias', 'vil_tri_prediction.weight', 'vil_tri_prediction.bias', 'vision_logit.weight', 'vision_logit.bias', 'linguisic_logit.weight', 'linguisic_logit.bias', 'bert.embeddings.task_embeddings.weight']\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_json_file(args.config_file)\n",
    "bert_weight_name = json.load(\n",
    "    open(\"config/\" + args.bert_model + \"_weight_name.json\", \"r\")\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.bert_model, do_lower_case=args.do_lower_case\n",
    ")\n",
    "\n",
    "config.track_temporal_features = args.track_temporal_features\n",
    "config.mean_layer = args.mean_layer\n",
    "config.max_temporal_memory_buffer = args.max_temporal_memory_buffer\n",
    "\n",
    "\n",
    "model = VILBertActionGrounding.from_pretrained(\n",
    "    args.from_pretrained, config=config, default_gpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from faster_rcnn.feature_extractor_new import featureExtractor\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "\n",
    "class DataLoader():\n",
    "    \"\"\"This class loads and preprocess the data set ALFRED for VilBERT.\n",
    "        Input: data.json and model for the feature extractor (FastRCnn)\n",
    "        Ouput: List of length = number of different instructions on our dataset. Each element of the list\n",
    "                is a dictionary containing a sequence of images + instruction (text), under the keys\n",
    "                [imgs] & [desc] respectively. [imgs] its a dictionary with keys [features], \n",
    "                [pos_enco] and [infos], gathering the already masked and tokenized features extracted from \n",
    "                the fasRCnn, a positional encoder of the bounding boxes and some additional information. \n",
    "                [desc] is a dictionary with keys [text_id],[modified_token] and [masked_lm_token], gathering\n",
    "                the tokenized instruction, the modification after making it and the masked_lm_token for VilBERT, \n",
    "                respectively. \"\"\"\n",
    "    \n",
    "    def __init__(self, json_path, model):\n",
    "        self.data = json.load(open(json_path, \"r\"))\n",
    "        self.tokenized_data = deepcopy(self.data)\n",
    "        self.model = model\n",
    "        \n",
    "    def get_processed_data(self):\n",
    "        return self.tokenized_data\n",
    "        \n",
    "    def extract_features(self):\n",
    "        print(\"  Extracting features...\")\n",
    "        for i, one_action_data in enumerate(self.tokenized_data):\n",
    "            print(\"    Action indx\", i)\n",
    "            f_extractor = featureExtractor(one_action_data[\"imgs\"], self.model)\n",
    "            features, positional_encoding, infos = f_extractor.extract_features() \n",
    "            self.tokenized_data[i][\"imgs\"] = {\"feat\":features,\"pos_enco\": positional_encoding, \"spatial\":[], \"image_mask\":[],\n",
    "                                              \"infos\":infos, \"co_attention_mask\":[]}\n",
    "        return self.tokenized_data\n",
    "    \n",
    "    def text_tokenizer(self):\n",
    "        \"\"\"We add the special tokens to the text (actions)\"\"\"\n",
    "        print(\"Tokenizing text...\")\n",
    "        for i, one_action_data in enumerate(self.tokenized_data):\n",
    "            text = '[CLS]' + one_action_data[\"desc\"][0] + '[SEP]'\n",
    "            self.length_text = len(text)\n",
    "            tokenized_text = tokenizer.tokenize(text)\n",
    "            tokenized_text = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "            segment_ids = [0] * len(tokenized_text)\n",
    "            input_mask = [1] * len(tokenized_text)\n",
    "            self.max_length = 37\n",
    "            if len(tokenized_text) < self.max_length:\n",
    "                # Note here we pad in front of the sentence\n",
    "                padding = [0] * (self.max_length - len(tokenized_text))\n",
    "                tokenized_text = tokenized_text + padding\n",
    "                input_mask += padding\n",
    "                segment_ids += padding\n",
    "                \n",
    "            self.tokenized_data[i][\"desc\"] = {\"tokenized_text\":tokenized_text, \n",
    "                                              \"input_mask\":input_mask,\"segment_ids\": segment_ids,\n",
    "                                              \"modified_token\":[], \"masked_lm_token\":[]}\n",
    "            \n",
    "    def img_tokenizer(self):\n",
    "        \"\"\"Add the spacial token IMG before each set of features extracted from an image\"\"\"\n",
    "        print(\"Tokenizing images...\")\n",
    "        self.extract_features()\n",
    "        for i, one_action_data in enumerate(self.tokenized_data):\n",
    "            for j in range(len(one_action_data[\"imgs\"][\"feat\"])):\n",
    "                \n",
    "                mean_pooled_feat = torch.mean(one_action_data[\"imgs\"][\"feat\"][j], 0).unsqueeze(0) #Equivalent to IMG special token\n",
    "                one_action_data[\"imgs\"][\"feat\"][j] = torch.cat((mean_pooled_feat, one_action_data[\"imgs\"][\"feat\"][j]), dim=0)\n",
    "                \n",
    "                one_action_data[\"imgs\"][\"infos\"][j][\"objects\"] = torch.cat((torch.tensor([-1]), one_action_data[\"imgs\"][\"infos\"][j][\"objects\"] ), dim=0)\n",
    "                one_action_data[\"imgs\"][\"pos_enco\"][j] = torch.cat((torch.zeros((1, one_action_data[\"imgs\"][\"pos_enco\"][j].shape[1])), one_action_data[\"imgs\"][\"pos_enco\"][j]), dim=0)\n",
    "                \n",
    "                boxes = one_action_data[\"imgs\"][\"infos\"][j][\"bbox\"]\n",
    "                image_w = one_action_data[\"imgs\"][\"infos\"][j][\"image_width\"]\n",
    "                image_h = one_action_data[\"imgs\"][\"infos\"][j][\"image_height\"]\n",
    "                \n",
    "                image_location = torch.zeros((boxes.shape[0], 5))\n",
    "                image_location[:,:4] = torch.from_numpy(boxes)\n",
    "                image_location[:,4] = (image_location[:,3] - image_location[:,1]) * (image_location[:,2] - image_location[:,0]) / (float(image_w) * float(image_h))\n",
    "                image_location[:,0] = image_location[:,0] / float(image_w)\n",
    "                image_location[:,1] = image_location[:,1] / float(image_h)\n",
    "                image_location[:,2] = image_location[:,2] / float(image_w)\n",
    "                image_location[:,3] = image_location[:,3] / float(image_h)\n",
    "                full_image_5D_encoding = torch.FloatTensor([[0, 0, 1, 1, 1]])\n",
    "                one_action_data[\"imgs\"][\"image_mask\"].append(torch.tensor([1] * (int(one_action_data[\"imgs\"][\"infos\"][j][\"num_boxes\"]+1))))\n",
    "                \n",
    "                spatial_img_location = torch.cat((full_image_5D_encoding, image_location), dim=0)\n",
    "                one_action_data[\"imgs\"][\"spatial\"].append(spatial_img_location) \n",
    "            \n",
    "    def mask_text(self):\n",
    "        \n",
    "        \"\"\"We will generate 2 new outputs from the Tokenized text: the modified token and the mask_lm_token\n",
    "            which will be stored in the dictionary in self.tokenized_data[\"desc\"] \n",
    "            under the keys (text_id, modified_token, mask_lm_token)\"\"\"\n",
    "        \n",
    "        for i, one_action_data in enumerate(self.tokenized_data):\n",
    "            \n",
    "            modified_token = deepcopy(one_action_data[\"desc\"][\"tokenized_text\"])\n",
    "            masked_lm_labels = -1*np.ones(len(modified_token)).astype(int)\n",
    "            type_modification = np.random.choice(np.array([\"MASK\", \"random\", \"unaltered\"]), p=[0.8, 0.1,0.1])\n",
    "            length_token = self.length_text - 2\n",
    "            num_masked_tokens = int(0.15*length_token)\n",
    "\n",
    "            if num_masked_tokens<1:\n",
    "                num_masked_tokens=1\n",
    "\n",
    "            if type_modification==\"MASK\":\n",
    "                for i in range(num_masked_tokens):\n",
    "                    indx = np.random.randint(length_token)\n",
    "                    modified_token[indx+1] = tokenizer.encode('MASK')[0]\n",
    "                    masked_lm_labels[indx+1] = tokenizer.encode('MASK')[0]\n",
    "\n",
    "            elif type_modification==\"random\":\n",
    "                for i in range(num_masked_tokens):\n",
    "                    indx = np.random.randint(length_token)\n",
    "                    modified_token[indx+1] = np.random.randint(30522) #size of vacbulary\n",
    "                    masked_lm_labels[indx+1] = np.random.randint(1024)\n",
    "            else:\n",
    "                for i in range(num_masked_tokens):\n",
    "                    indx = np.random.randint(length_token)\n",
    "                    modified_token[indx+1] = one_action_data[\"desc\"][\"tokenized_text\"][indx+1] #size of vacbulary\n",
    "                    masked_lm_labels[indx+1] = one_action_data[\"desc\"][\"tokenized_text\"][indx+1]\n",
    "            \n",
    "            one_action_data[\"desc\"][\"modified_token\"] = modified_token\n",
    "            one_action_data[\"desc\"][\"masked_lm_token\"] = masked_lm_labels\n",
    "            \n",
    "    \n",
    "    def mask_img(self):\n",
    "        \n",
    "        \"\"\"\"We will mask the 15% of the patches features with 90% probability to zeroed features \n",
    "        and 10% unalteres\"\"\"\n",
    "        \n",
    "        for i, one_action_data in enumerate(self.tokenized_data):\n",
    "            for f in range(len(one_action_data[\"imgs\"][\"feat\"])):\n",
    "                length_feat = one_action_data[\"imgs\"][\"feat\"][f].shape[0]\n",
    "                type_modification = np.random.choice(np.array([\"zeros\", \"unaltered\"]), p=[0.9, 0.1])\n",
    "                num_masked_tokens = int(0.15*length_feat)\n",
    "\n",
    "                if num_masked_tokens<1:\n",
    "                    num_masked_tokens=1\n",
    "\n",
    "                if type_modification==\"zeros\":\n",
    "                    for _ in range(num_masked_tokens):\n",
    "                        i = np.random.randint(length_feat)\n",
    "                        one_action_data[\"imgs\"][\"feat\"][f][i] = torch.zeros((1, one_action_data[\"imgs\"][\"feat\"][f].shape[1]))\n",
    "\n",
    "    \n",
    "    def get_data_masked_train(self):\n",
    "        \"\"\"This function executes tranforms the text into tockens, the extractor of features\n",
    "        the masking in the text and image\"\"\"\n",
    "        self.text_tokenizer()\n",
    "        self.img_tokenizer()\n",
    "        self.mask_text()\n",
    "        self.mask_img()\n",
    "        data = self.get_processed_data()\n",
    "        for instruction, data_point in enumerate(data):\n",
    "            #To avoid issues with last action STOP\n",
    "            if instruction == len(data)-1:\n",
    "                break\n",
    "            for type_data_key, type_data_value in data_point.items():\n",
    "                for key, value in type_data_value.items():\n",
    "                    if type_data_key == \"imgs\":\n",
    "                        if key == \"infos\":\n",
    "                            continue\n",
    "                        if key == \"co_attention_mask\":\n",
    "                            type_data_value[key].append(torch.zeros((data[0][\"imgs\"][\"feat\"].shape[0], self.max_length)))\n",
    "                        if key == \"image_mask\":\n",
    "                            cat =  value[0]\n",
    "                            for i in range(1,len(value)):\n",
    "                                cat = torch.cat((cat, value[i]), dim=0)\n",
    "                            type_data_value[key] = cat\n",
    "                        else:\n",
    "                            type_data_value[key] = torch.cat(value, dim=0)\n",
    "                    else: \n",
    "                        type_data_value[key] = torch.tensor(value)\n",
    "                        \n",
    "        return (data[0][\"imgs\"][\"feat\"], data[0][\"imgs\"][\"pos_enco\"], data[0][\"imgs\"][\"spatial\"],data[0][\"imgs\"][\"image_mask\"],\n",
    "                data[0][\"desc\"][\"tokenized_text\"], data[0][\"desc\"][\"modified_token\"],data[0][\"desc\"][\"masked_lm_token\"],\n",
    "                 data[0][\"desc\"][\"input_mask\"], data[0][\"desc\"][\"segment_ids\"], data[0][\"imgs\"][\"co_attention_mask\"], data[0][\"imgs\"][\"infos\"])\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text...\n",
      "Tokenizing images...\n",
      "  Extracting features...\n",
      "    Action indx 0\n",
      "    Action indx 1\n",
      "    Action indx 2\n",
      "    Action indx 3\n"
     ]
    }
   ],
   "source": [
    "frcnn_model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "DataLoader = DataLoader(\"short_json_data.json\", frcnn_model)\n",
    "data = DataLoader.get_data_masked_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_masked, pos_enc, spatial, image_mask, tokenized_text, masked_text, masked_lm_token, input_mask, segment_ids, co_attention_mask, infos = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 6144])\n",
      "tensor([  101,  2175,  2000,  1996, 13065,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0])\n",
      "tensor([  101,  2175,  2000,  1996, 13065,   102,     0,     0,     0,     0,\n",
      "            0,  7308,  7308,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "print(features_masked.shape)\n",
    "print(tokenized_text)\n",
    "print(masked_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33])\n"
     ]
    }
   ],
   "source": [
    "image_target = []\n",
    "for i in range(len(infos)):\n",
    "    image_target.append(infos[i][\"objects\"])\n",
    "\n",
    "image_target = torch.cat(image_target, dim=0)\n",
    "print(image_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                    lr=args.learning_rate,\n",
    "                    eps=args.adam_epsilon,\n",
    "                    betas=(0.9, 0.98),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict(model.named_parameters()).items():\n",
    "    if not value.requires_grad:\n",
    "        print(\"This parameter does have grad\", key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features after adding pos enc -> torch.Size([1, 33, 2048])\n",
      "loss:  tensor(1.6649, grad_fn=<AddBackward0>)\n",
      "Image features after adding pos enc -> torch.Size([1, 33, 2048])\n",
      "loss:  tensor(2.6292, grad_fn=<AddBackward0>)\n",
      "Image features after adding pos enc -> torch.Size([1, 33, 2048])\n",
      "loss:  tensor(1.6354, grad_fn=<AddBackward0>)\n",
      "Image features after adding pos enc -> torch.Size([1, 33, 2048])\n",
      "loss:  tensor(1.5269, grad_fn=<AddBackward0>)\n",
      "Image features after adding pos enc -> torch.Size([1, 33, 2048])\n",
      "loss:  tensor(1.6374, grad_fn=<AddBackward0>)\n",
      "Image features after adding pos enc -> torch.Size([1, 33, 2048])\n",
      "loss:  tensor(1.5980, grad_fn=<AddBackward0>)\n",
      "Image features after adding pos enc -> torch.Size([1, 33, 2048])\n",
      "loss:  tensor(1.5098, grad_fn=<AddBackward0>)\n",
      "Image features after adding pos enc -> torch.Size([1, 33, 2048])\n",
      "loss:  tensor(1.4486, grad_fn=<AddBackward0>)\n",
      "Image features after adding pos enc -> torch.Size([1, 33, 2048])\n",
      "loss:  tensor(1.4301, grad_fn=<AddBackward0>)\n",
      "Image features after adding pos enc -> torch.Size([1, 33, 2048])\n",
      "loss:  tensor(1.3781, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    pred_t, pred_v, att = model(input_ids = tokenized_text.unsqueeze(0).cpu(),\n",
    "                            image_feat = features_masked.unsqueeze(0).cpu(), # Linear(2048*config.max_temporal_memory_buffer, 2048)\n",
    "                            image_loc = spatial.unsqueeze(0).cpu(),  #Linear(in_features=5, out_features=1024, bias=True)\n",
    "                            image_pos_input = pos_enc.unsqueeze(0).cpu(),   #Linear(7, 2048)/(6, 2048)\n",
    "                            token_type_ids = segment_ids.unsqueeze(0).cpu(), \n",
    "                            attention_mask = input_mask.unsqueeze(0).cpu(), \n",
    "                            image_attention_mask = image_mask.unsqueeze(0).cpu(), \n",
    "                            co_attention_mask = co_attention_mask.unsqueeze(0).cpu(),\n",
    "                            masked_lm_labels = masked_lm_token.unsqueeze(0).cpu(), \n",
    "                            image_label = None,\n",
    "                            image_target = None,\n",
    "                            next_sentence_label=None,\n",
    "                            output_all_attention_masks=True)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    masked_lm_loss = model.lang_criterion(pred_t.view(-1, 30522), masked_lm_token.cpu().view(-1))\n",
    "    img_loss = model.vis_criterion(pred_v.view(-1, 91), image_target.cpu()) # why dim 2 (to check) \n",
    "    loss = masked_lm_loss + img_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"loss: \", loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_masked_lang = model.lang_criterion(pred_t, labels_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([None, None, None, None, None, None, None, None, None, None, None, None], [None, None, None, None, None, None], [None, None, None, None, None, None])\n"
     ]
    }
   ],
   "source": [
    "print((att))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.stack(feature_list, dim=0).float().cuda()\n",
    "    spatials = torch.stack(image_location_list, dim=0).float().cuda()\n",
    "    image_mask = torch.stack(image_mask_list, dim=0).byte().cuda()\n",
    "    co_attention_mask = torch.zeros((num_image, num_boxes, max_length)).cuda()\n",
    "\n",
    "    prediction(text, features, spatials, segment_ids, input_mask, image_mask, co_attention_mask, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[1][\"imgs\"][\"feat\"].shape)\n",
    "concate = torch.cat((data[1][\"imgs\"][\"feat\"].unsqueeze(0), data[0][\"imgs\"][\"feat\"].unsqueeze(0)), dim=0)\n",
    "print(concate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Structure of data: \")\n",
    "print(\" 路 Number of instructions: \", len(data))\n",
    "print(\"  路 Per instruction we have: \", data[0].keys())\n",
    "print(\"    路 The 'desc' of the instruction has: \", data[0][\"desc\"].keys())\n",
    "print(\"       - lists of length tokenized text -->\",len(data[0][\"desc\"][\"tokenized_text\"]))\n",
    "print(\"    路 The 'imgs' of the instruction has:\", data[0][\"imgs\"].keys())\n",
    "for k, v in data[0][\"imgs\"].items():\n",
    "    print(k, v)\n",
    "    #print(len(data[0][\"imgs\"][\"spatial\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict(model.named_parameters()).items():\n",
    "    if not value.requires_grad:\n",
    "        print(\"This parameter does have grad\", key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda: model = model.cuda(0)\n",
    "#Why do we initialize Tokenizer again?\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.bert_model, do_lower_case=args.do_lower_case\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenize( text):\n",
    "    text = '[CLS]' + text + '[SEP]'\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    return indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"swimming elephant\"\n",
    "indexed_tokens = bert_tokenize(text)\n",
    "#indexed_tokens, modified_indexed_tokens, masked_lm_labels = pretask_mask_lang_tokens(indexed_tokens)\n",
    "\n",
    "query = '[CLS]' + text + '[SEP]'\n",
    "tokens = tokenizer.encode(query)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(question, features, spatials, segment_ids, input_mask, image_mask, co_attention_mask, task_tokens, ):\n",
    "    print('input question size question: ', question.shape)\n",
    "    pos_enc_input = torch.FloatTensor([0,1,2,3,4,5])\n",
    "    print(question.shape)\n",
    "    masked_lm_loss, masked_img_loss, prediction_t, prediction_v, all_attention_mask = model(\n",
    "        input_ids=question, image_feat=features, image_loc=spatials,image_pos_input=pos_enc_input, output_all_attention_masks=True\n",
    "    )\n",
    "    return masked_lm_loss, masked_img_loss, prediction_t, prediction_v, all_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_prediction(query, task, features, infos):\n",
    "\n",
    "#     print(query)\n",
    "#     query = '[CLS]' + query + '[SEP]'\n",
    "#     tokens = tokenizer.encode(query)\n",
    "#     print(tokens)\n",
    "    #tokens= \n",
    "    #tokens = tokenizer.add_special_tokens_single_sentence(tokens)\n",
    "    \n",
    "    tokens = bert_tokenize(query)\n",
    "    segment_ids = [0] * len(tokens)\n",
    "    input_mask = [1] * len(tokens)\n",
    "\n",
    "    max_length = 37\n",
    "    if len(tokens) < max_length:\n",
    "        # Note here we pad in front of the sentence\n",
    "        padding = [0] * (max_length - len(tokens))\n",
    "        tokens = tokens + padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "    text = torch.from_numpy(np.array(tokens)).cuda().unsqueeze(0)\n",
    "    input_mask = torch.from_numpy(np.array(input_mask)).cuda().unsqueeze(0)\n",
    "    segment_ids = torch.from_numpy(np.array(segment_ids)).cuda().unsqueeze(0)\n",
    "    task = torch.from_numpy(np.array(task)).cuda().unsqueeze(0)\n",
    "\n",
    "    num_image = len(infos)\n",
    "\n",
    "    feature_list = []\n",
    "    image_location_list = []\n",
    "    image_mask_list = []\n",
    "    for i in range(num_image):\n",
    "        image_w = infos[i]['image_width']\n",
    "        image_h = infos[i]['image_height']\n",
    "        feature = features[i]\n",
    "        num_boxes = feature.shape[0] #first dim size = number boxes\n",
    "\n",
    "        g_feat = torch.sum(feature, dim=0) / num_boxes # Mean of features of all the selected regions\n",
    "        num_boxes = num_boxes + 1\n",
    "        feature = torch.cat([g_feat.view(1,-1), feature], dim=0)\n",
    "        boxes = infos[i]['bbox']\n",
    "        image_location = np.zeros((boxes.shape[0], 5), dtype=np.float32)\n",
    "        image_location[:,:4] = boxes\n",
    "        image_location[:,4] = (image_location[:,3] - image_location[:,1]) * (image_location[:,2] - image_location[:,0]) / (float(image_w) * float(image_h))\n",
    "        image_location[:,0] = image_location[:,0] / float(image_w)\n",
    "        image_location[:,1] = image_location[:,1] / float(image_h)\n",
    "        image_location[:,2] = image_location[:,2] / float(image_w)\n",
    "        image_location[:,3] = image_location[:,3] / float(image_h)\n",
    "        g_location = np.array([0,0,1,1,1])\n",
    "        image_location = np.concatenate([np.expand_dims(g_location, axis=0), image_location], axis=0)\n",
    "        image_mask = [1] * (int(num_boxes))\n",
    "\n",
    "        feature_list.append(feature)\n",
    "        image_location_list.append(torch.tensor(image_location))\n",
    "        image_mask_list.append(torch.tensor(image_mask))\n",
    "\n",
    "    features = torch.stack(feature_list, dim=0).float().cuda()\n",
    "    spatials = torch.stack(image_location_list, dim=0).float().cuda()\n",
    "    image_mask = torch.stack(image_mask_list, dim=0).byte().cuda()\n",
    "    co_attention_mask = torch.zeros((num_image, num_boxes, max_length)).cuda()\n",
    "#     print(\"text: \", text.shape)\n",
    "#     print(\"feat: \", features.shape)\n",
    "#     print(\"spatials: \", spatials.shape)\n",
    "#     print(\"segments_id: \", segment_ids)\n",
    "#     print(\"input_mask: \", input_mask)\n",
    "#     print(\"image_mask: \", image_mask)\n",
    "#     print(\"coatenttion_mask: \", co_attention_mask)\n",
    "    return prediction(text, features, spatials, segment_ids, input_mask, image_mask, co_attention_mask, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frcnn_model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# read image \n",
    "#pic = \"faster_rcnn/test2.png\"\n",
    "#pic1 = \"faster_rcnn/test.png\"\n",
    "#image_paths = [pic, pic1]\n",
    "\n",
    "\n",
    "\n",
    "image_path = ['demo/1.jpg']\n",
    "features, pos_enc, infos = featureExtractor(image_path, frcnn_model).extract_features()\n",
    "print(\"features: \", type(features))\n",
    "print(\"infos: \", infos)\n",
    "#features, infos = f_extractor.extract_features(image_path, frcnn_model)\n",
    "\n",
    "\n",
    "img = PIL.Image.open(image_path[0]).convert('RGB')\n",
    "img = torch.tensor(np.array(img))\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "    \n",
    "query = \"swimming elephant\"\n",
    "task = [9]\n",
    "masked_lm_loss, masked_img_loss, prediction_t, prediction_v, all_attention_mask = custom_prediction(query, task, features, infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0][\"imgs\"][\"pos_enco\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_lm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " prediction_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "config.visual_target = args.visual_target\n",
    "\n",
    "optimizer_grouped_parameters = []\n",
    "for key, value in dict(model.named_parameters()).items():\n",
    "    if value.requires_grad:\n",
    "        if \"cls\" in key:\n",
    "            lr = args.learning_rate\n",
    "        else:\n",
    "            lr = args.learning_rate * 0.1\n",
    "        if any(nd in key for nd in no_decay): # No decay\n",
    "            optimizer_grouped_parameters += [\n",
    "                {\"params\": [value],\n",
    "                 \"lr\": lr,\n",
    "                 \"weight_decay\": 0.0}\n",
    "            ]\n",
    "        \n",
    "        elif not any(nd in key for nd in no_decay):\n",
    "            optimizer_grouped_parameters += [\n",
    "                {\"params\": [value],\n",
    "                 \"lr\": lr,\n",
    "                 \"weight_decay\": 0.01}\n",
    "            ]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=args.learning_rate,\n",
    "    eps=args.adam_epsilon,\n",
    "    betas=(0.9, 0.98),\n",
    ")\n",
    "\n",
    "num_dataset_points = 19\n",
    "\n",
    "num_train_optimization_steps = int(\n",
    "    num_dataset_points\n",
    "    / args.train_batch_size\n",
    "    / args.gradient_accumulation_steps\n",
    ") * (args.num_train_epochs - args.start_epoch)\n",
    "\n",
    "scheduler = WarmupLinearSchedule(\n",
    "    optimizer,\n",
    "    warmup_steps=args.warmup_proportion * num_train_optimization_steps,\n",
    "    t_total=num_train_optimization_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.cuda()\n",
    "# for state in optimizer.state.values():\n",
    "#     for k, v in state.items():\n",
    "#         if torch.is_tensor(v):\n",
    "#             state[k] = v.cuda()\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"Num examples = %d\", num_dataset_points)\n",
    "logger.info(\"Batch size = %d\", args.train_batch_size)\n",
    "logger.info(\"Num steps = %d\", num_train_optimization_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startIterID = 0\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(int(args.start_epoch), int(args.num_train_epochs)):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataset):\n",
    "        iterId = startIterID + step + (epochId * len(train_dataset))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('vilbert-mt': conda)",
   "language": "python",
   "name": "python361064bitvilbertmtconda7bb1bd3e955c469187a4ae8b5b7d6907"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
