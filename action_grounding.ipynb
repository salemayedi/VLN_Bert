{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/drissh/lab/lab/lib/python3.6/site-packages/tensorpack/callbacks/hooks.py:17: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/drissh/lab/lab/lib/python3.6/site-packages/tensorpack/tfutils/optimizer.py:18: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/drissh/lab/lab/lib/python3.6/site-packages/tensorpack/tfutils/sesscreate.py:20: The name tf.train.SessionCreator is deprecated. Please use tf.compat.v1.train.SessionCreator instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import json\n",
    "import torch\n",
    "from types import SimpleNamespace\n",
    "from vilbert.vilbert import VILBertActionGrounding, BertConfig\n",
    "from pytorch_transformers.tokenization_bert import BertTokenizer\n",
    "from pytorch_transformers.optimization import AdamW, WarmupLinearSchedule\n",
    "import torch.distributed as dist\n",
    "from VLN_config import config as args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import argparse\n",
    "import glob\n",
    "import pdb\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from faster_rcnn import feature_extractor_new as f_extractor\n",
    "from faster_rcnn.feature_extractor_new import featureExtractor\n",
    "#%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(adam_epsilon=1e-08, baseline=False, bert_model='bert-base-uncased', best_features=10, clean_train_sets=True, config_file='config/bert_base_6layer_6conect.json', distributed=False, do_lower_case=True, dynamic_attention=False, from_pretrained='save/multitask_model/multi_task_model.bin', gradient_accumulation_steps=1, img_weight=1, in_memory=False, learning_rate=0.0001, local_rank=-1, max_temporal_memory_buffer=3, mean_layer=False, num_key_frames=3, num_train_epochs=10.0, num_workers=0, objective=1, predict_feature=False, save_name='', seed=42, split='mteval', start_epoch=0, task_specific_tokens=True, tasks='1', threshold_similarity=0.7, track_temporal_features=True, train_batch_size=1, visual_target=0, warmup_proportion=0.1, without_coattention=False)\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/21/2020 20:54:18 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/drissh/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "07/21/2020 20:54:18 - INFO - vilbert.utils -   loading weights file save/multitask_model/multi_task_model.bin\n",
      "07/21/2020 20:54:28 - INFO - vilbert.utils -   Weights of VILBertActionGrounding not initialized from pretrained model: ['positional_enc.weight', 'positional_enc.bias', 'img_emb_mean.weight', 'img_emb_mean.bias', 'action_cls.predictions.bias', 'action_cls.predictions.transform.dense.weight', 'action_cls.predictions.transform.dense.bias', 'action_cls.predictions.transform.LayerNorm.weight', 'action_cls.predictions.transform.LayerNorm.bias', 'action_cls.predictions.decoder.weight', 'action_cls.bi_seq_relationship.weight', 'action_cls.bi_seq_relationship.bias', 'action_cls.imagePredictions.transform.dense.weight', 'action_cls.imagePredictions.transform.dense.bias', 'action_cls.imagePredictions.transform.LayerNorm.weight', 'action_cls.imagePredictions.transform.LayerNorm.bias', 'action_cls.imagePredictions.decoder.weight', 'action_cls.imagePredictions.decoder.bias']\n",
      "07/21/2020 20:54:28 - INFO - vilbert.utils -   Weights from pretrained model not used in VILBertActionGrounding: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.bi_seq_relationship.weight', 'cls.bi_seq_relationship.bias', 'cls.imagePredictions.transform.dense.weight', 'cls.imagePredictions.transform.dense.bias', 'cls.imagePredictions.transform.LayerNorm.weight', 'cls.imagePredictions.transform.LayerNorm.bias', 'cls.imagePredictions.decoder.weight', 'cls.imagePredictions.decoder.bias', 'vil_prediction.logit_fc.0.weight', 'vil_prediction.logit_fc.0.bias', 'vil_prediction.logit_fc.2.weight', 'vil_prediction.logit_fc.2.bias', 'vil_prediction.logit_fc.3.weight', 'vil_prediction.logit_fc.3.bias', 'vil_prediction_gqa.logit_fc.0.weight', 'vil_prediction_gqa.logit_fc.0.bias', 'vil_prediction_gqa.logit_fc.2.weight', 'vil_prediction_gqa.logit_fc.2.bias', 'vil_prediction_gqa.logit_fc.3.weight', 'vil_prediction_gqa.logit_fc.3.bias', 'vil_binary_prediction.logit_fc.0.weight', 'vil_binary_prediction.logit_fc.0.bias', 'vil_binary_prediction.logit_fc.2.weight', 'vil_binary_prediction.logit_fc.2.bias', 'vil_binary_prediction.logit_fc.3.weight', 'vil_binary_prediction.logit_fc.3.bias', 'vil_logit.weight', 'vil_logit.bias', 'vil_tri_prediction.weight', 'vil_tri_prediction.bias', 'vision_logit.weight', 'vision_logit.bias', 'linguisic_logit.weight', 'linguisic_logit.bias', 'bert.embeddings.task_embeddings.weight']\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_json_file(args.config_file)\n",
    "bert_weight_name = json.load(\n",
    "    open(\"config/\" + args.bert_model + \"_weight_name.json\", \"r\")\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.bert_model, do_lower_case=args.do_lower_case\n",
    ")\n",
    "\n",
    "config.track_temporal_features = args.track_temporal_features\n",
    "config.mean_layer = args.mean_layer\n",
    "config.max_temporal_memory_buffer = args.max_temporal_memory_buffer\n",
    "\n",
    "model = VILBertActionGrounding.from_pretrained(\n",
    "    args.from_pretrained, config=config, default_gpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from faster_rcnn.feature_extractor_new import featureExtractor\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "\n",
    "class DataLoader():\n",
    "    \"\"\"This class loads and preprocess the data set ALFRED for VilBERT.\n",
    "        Input: data.json and model for the feature extractor (FastRCnn)\n",
    "        Ouput: List of length = number of different instructions on our dataset. Each element of the list\n",
    "                is a dictionary containing a sequence of images + instruction (text), under the keys\n",
    "                [imgs] & [desc] respectively. [imgs] its a dictionary with keys [features], \n",
    "                [pos_enco] and [infos], gathering the already masked and tokenized features extracted from \n",
    "                the fasRCnn, a positional encoder of the bounding boxes and some additional information. \n",
    "                [desc] is a dictionary with keys [text_id],[modified_token] and [masked_lm_token], gathering\n",
    "                the tokenized instruction, the modification after making it and the masked_lm_token for VilBERT, \n",
    "                respectively. \"\"\"\n",
    "    \n",
    "    def __init__(self, json_path, model):\n",
    "        self.data = json.load(open(json_path, \"r\"))\n",
    "        self.tokenized_data = [{\"imgs\": [], \"desc\": []}] * len(self.data)\n",
    "        self.model = model\n",
    "        \n",
    "    def get_processed_data(self):\n",
    "        return self.tokenized_data\n",
    "    \n",
    "    def add_randomly_selected_box(self, features, positional_encoding, infos):\n",
    "        i = np.random.randint(features.shape[0])\n",
    "        features = torch.cat((features, features[i].reshape(1, -1)), dim=0)\n",
    "        positional_encoding = torch.cat((positional_encoding, positional_encoding[i].reshape(1, -1)), dim=0)\n",
    "        infos[\"bbox\"] = np.concatenate((infos[\"bbox\"], infos[\"bbox\"][i].reshape(1, -1)), axis=0)\n",
    "        infos[\"pos_enc\"] = np.concatenate((infos[\"pos_enc\"], infos[\"pos_enc\"][i].reshape(1, -1)), axis=0)\n",
    "        infos[\"objects\"] = torch.cat((infos[\"objects\"], infos[\"objects\"][i].unsqueeze(0)), dim=0)\n",
    "        infos[\"cls_prob\"] = np.concatenate((infos[\"cls_prob\"], infos[\"cls_prob\"][i].reshape(1)), axis=0)\n",
    "        infos[\"num_boxes\"]+=1\n",
    "        return features, positional_encoding, infos\n",
    "    \n",
    "    def flatten_to_one_img(self, features, positional_encoding, infos):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        features = torch.cat(features, dim=0)\n",
    "        positional_encoding = torch.cat(positional_encoding, dim=0)\n",
    "        import pdb; pdb.set_trace()\n",
    "        return [features], positional_encoding, infos\n",
    "    \n",
    "    \n",
    "    def extract_features(self):\n",
    "        print(\"  Extracting features...\")\n",
    "        for i, one_action_data in enumerate(self.data):\n",
    "            print(\"    Action indx\", i)\n",
    "            num_frames = len(one_action_data[\"imgs\"])\n",
    "            k = args.num_key_frames\n",
    "            hb = num_frames\n",
    "            step = num_frames//args.num_key_frames\n",
    "            features, positional_encoding, infos = [], [], []\n",
    "            print(\"We have %d frames\" % num_frames)\n",
    "            while(hb > 0):\n",
    "                if k!=1:\n",
    "                    lb = max(hb-step,0)\n",
    "                    print(\"slice from %d -> %d with %d\" % (lb, hb, hb - lb))\n",
    "                    seq = one_action_data[\"imgs\"][int(lb):int(hb)]\n",
    "                    hb = hb-step\n",
    "                else:\n",
    "                    print(\"slice from %d -> %d with %d\" % (0, hb, hb))\n",
    "                    seq = one_action_data[\"imgs\"][:int(hb)]\n",
    "                    hb = -1\n",
    "                f_extractor = featureExtractor(seq, self.model, \n",
    "                                               temporal_buffer_size=len(seq))\n",
    "                \n",
    "                feat, pos_enc, info = f_extractor.extract_features()\n",
    "                feat, pos_enc, info = feat[-1], pos_enc[-1], info[-1]\n",
    "                \n",
    "                while(feat.shape[0] < args.best_features):\n",
    "                    feat, pos_enc, info = self.add_randomly_selected_box(feat, pos_enc, info)\n",
    "                                  \n",
    "                features.append(feat)\n",
    "                positional_encoding.append(pos_enc)\n",
    "                infos.append(info)\n",
    "                \n",
    "                k-=1\n",
    "#             import pdb; pdb.set_trace()\n",
    "            features.reverse()\n",
    "            positional_encoding.reverse()\n",
    "            infos.reverse()\n",
    "            # Flatten to one img so that, we have r_0..r_k*nboxes as if it was one image\n",
    "            # To be applied for the 3 lists above\n",
    "            features, positional_encoding, infos = self.flatten_to_one_img(features, positional_encoding, infos)\n",
    "            \n",
    "            \n",
    "            self.tokenized_data[i][\"imgs\"] = {\"feat\":features ,\"pos_enco\": positional_encoding, \n",
    "                                              \"spatial\":[], \"image_mask\":[],\n",
    "                                              \"infos\":infos, \"co_attention_mask\":[], \n",
    "                                              \"masked_img_labels\" : [[] for _ in range(len(features))]}\n",
    "        \n",
    "#         return self.tokenized_data\n",
    "    \n",
    "    \n",
    "    def text_tokenizer(self):\n",
    "        \"\"\"We add the special tokens to the text (actions)\"\"\"\n",
    "        print(\"Tokenizing text...\")\n",
    "        for i, one_action_data in enumerate(self.data):\n",
    "            text = '[CLS]' + one_action_data[\"desc\"][0] + '[SEP]'\n",
    "            self.length_text = len(text)\n",
    "            tokenized_text = tokenizer.tokenize(text)\n",
    "            tokenized_text = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "            segment_ids = [0] * len(tokenized_text)\n",
    "            input_mask = [1] * len(tokenized_text)\n",
    "            self.max_length = 37\n",
    "            if len(tokenized_text) < self.max_length:\n",
    "                # Note here we pad in front of the sentence\n",
    "                padding = [0] * (self.max_length - len(tokenized_text))\n",
    "                tokenized_text = tokenized_text + padding\n",
    "                input_mask += padding\n",
    "                segment_ids += padding\n",
    "                \n",
    "            self.tokenized_data[i][\"desc\"] = {\"tokenized_text\":tokenized_text, \n",
    "                                              \"input_mask\":input_mask,\"segment_ids\": segment_ids,\n",
    "                                              \"modified_token\":[], \"masked_lm_token\":[]}\n",
    "            \n",
    "    def img_tokenizer(self):\n",
    "        \"\"\"Add the spacial token IMG before each set of features extracted from an image\"\"\"\n",
    "        self.extract_features()\n",
    "        for i, one_action_data in enumerate(self.tokenized_data):\n",
    "            for j in range(len(one_action_data[\"imgs\"][\"feat\"])):\n",
    "                mean_pooled_feat = torch.mean(one_action_data[\"imgs\"][\"feat\"][j], 0).unsqueeze(0) #Equivalent to IMG special token\n",
    "                one_action_data[\"imgs\"][\"feat\"][j] = torch.cat((mean_pooled_feat, one_action_data[\"imgs\"][\"feat\"][j]), dim=0)\n",
    "                \n",
    "                one_action_data[\"imgs\"][\"infos\"][j][\"objects\"] = torch.cat((torch.tensor([-1]), one_action_data[\"imgs\"][\"infos\"][j][\"objects\"] ), dim=0)\n",
    "                one_action_data[\"imgs\"][\"pos_enco\"][j] = torch.cat((torch.zeros((1, one_action_data[\"imgs\"][\"pos_enco\"][j].shape[1])), one_action_data[\"imgs\"][\"pos_enco\"][j]), dim=0)\n",
    "                \n",
    "                boxes = one_action_data[\"imgs\"][\"infos\"][j][\"bbox\"]\n",
    "                image_w = one_action_data[\"imgs\"][\"infos\"][j][\"image_width\"]\n",
    "                image_h = one_action_data[\"imgs\"][\"infos\"][j][\"image_height\"]\n",
    "                \n",
    "                image_location = torch.zeros((boxes.shape[0], 5))\n",
    "                image_location[:,:4] = torch.from_numpy(boxes)\n",
    "                image_location[:,4] = (image_location[:,3] - image_location[:,1]) * (image_location[:,2] - image_location[:,0]) / (float(image_w) * float(image_h))\n",
    "                image_location[:,0] = image_location[:,0] / float(image_w)\n",
    "                image_location[:,1] = image_location[:,1] / float(image_h)\n",
    "                image_location[:,2] = image_location[:,2] / float(image_w)\n",
    "                image_location[:,3] = image_location[:,3] / float(image_h)\n",
    "                full_image_5D_encoding = torch.FloatTensor([[0, 0, 1, 1, 1]])\n",
    "                one_action_data[\"imgs\"][\"image_mask\"].append(torch.tensor([1] * (int(one_action_data[\"imgs\"][\"infos\"][j][\"num_boxes\"]+1))))\n",
    "                \n",
    "                spatial_img_location = torch.cat((full_image_5D_encoding, image_location), dim=0)\n",
    "                one_action_data[\"imgs\"][\"spatial\"].append(spatial_img_location)\n",
    "            \n",
    "    def mask_text(self):\n",
    "        \n",
    "        \"\"\"We will generate 2 new outputs from the Tokenized text: the modified token and the mask_lm_token\n",
    "            which will be stored in the dictionary in self.tokenized_data[\"desc\"] \n",
    "            under the keys (text_id, modified_token, mask_lm_token)\"\"\"\n",
    "        \n",
    "        for i, one_action_data in enumerate(self.tokenized_data):\n",
    "            \n",
    "            modified_token = deepcopy(one_action_data[\"desc\"][\"tokenized_text\"])\n",
    "            masked_lm_labels = -1*np.ones(len(modified_token)).astype(int)\n",
    "            type_modification = np.random.choice(np.array([\"MASK\", \"random\", \"unaltered\"]), p=[0.8, 0.1,0.1])\n",
    "            length_token = self.length_text - 2\n",
    "            num_masked_tokens = int(0.15*length_token)\n",
    "            if num_masked_tokens<1:\n",
    "                num_masked_tokens=1\n",
    "\n",
    "            if type_modification==\"MASK\":\n",
    "                for i in range(num_masked_tokens):\n",
    "                    indx = np.random.randint(length_token)\n",
    "                    modified_token[indx+1] = tokenizer.encode('[MASK]')[0]\n",
    "                    masked_lm_labels[indx+1] = one_action_data[\"desc\"][\"tokenized_text\"][indx+1]\n",
    "\n",
    "            elif type_modification==\"random\":\n",
    "                for i in range(num_masked_tokens):\n",
    "                    indx = np.random.randint(length_token)\n",
    "                    modified_token[indx+1] = np.random.randint(30522) \n",
    "                    masked_lm_labels[indx+1] = one_action_data[\"desc\"][\"tokenized_text\"][indx+1]\n",
    "            else:\n",
    "                for i in range(num_masked_tokens):\n",
    "                    indx = np.random.randint(length_token)\n",
    "#                     modified_token[indx+1] = one_action_data[\"desc\"][\"tokenized_text\"][indx+1] \n",
    "                    masked_lm_labels[indx+1] = one_action_data[\"desc\"][\"tokenized_text\"][indx+1]\n",
    "            \n",
    "            one_action_data[\"desc\"][\"modified_token\"] = modified_token\n",
    "            one_action_data[\"desc\"][\"masked_lm_token\"] = masked_lm_labels\n",
    "            \n",
    "    \n",
    "    def mask_img(self):\n",
    "        \n",
    "        \"\"\"\"We will mask the 15% of the patches features with 90% probability to zeroed features \n",
    "        and 10% unalteres\"\"\"\n",
    "        \n",
    "        for _ , one_action_data in enumerate(self.tokenized_data):\n",
    "            for f in range(len(one_action_data[\"imgs\"][\"feat\"])):\n",
    "                length_feat = one_action_data[\"imgs\"][\"feat\"][f].shape[0] \n",
    "                type_modification = np.random.choice(np.array([\"zeros\", \"unaltered\"]), p=[0.9, 0.1])\n",
    "                masked_im_labels = -1*np.ones(length_feat).astype(int)\n",
    "                num_masked_tokens = int(0.15*(length_feat-1)) # Ignore IMG Token \n",
    "                if num_masked_tokens<1:\n",
    "                    num_masked_tokens=1\n",
    "                if type_modification==\"zeros\":\n",
    "                    for _ in range(num_masked_tokens): # Ignore IMG Token \n",
    "                        i = np.random.randint(length_feat-1)\n",
    "                        # Masking box j in image f\n",
    "                        one_action_data[\"imgs\"][\"feat\"][f][i+1] = torch.zeros((1, one_action_data[\"imgs\"][\"feat\"][f].shape[1]))\n",
    "                        masked_im_labels[i+1] = one_action_data[\"imgs\"][\"infos\"][f][\"objects\"][i+1].item()\n",
    "                one_action_data[\"imgs\"][\"masked_img_labels\"][f] = torch.from_numpy(masked_im_labels)\n",
    "    \n",
    "    def get_data_masked_train(self):\n",
    "        \"\"\"This function executes tranforms the text into tockens, the extractor of features\n",
    "        the masking in the text and image\"\"\"\n",
    "        self.text_tokenizer()\n",
    "        self.img_tokenizer()\n",
    "        assert False\n",
    "        self.mask_text()\n",
    "        self.mask_img()\n",
    "        data = self.get_processed_data()\n",
    "        for instruction, data_point in enumerate(data):\n",
    "            #To avoid issues with last action STOP\n",
    "            if instruction == len(data)-1:\n",
    "                break\n",
    "            for type_data_key, type_data_value in data_point.items(): # Here IMGS or DESC\n",
    "                for key, value in type_data_value.items():\n",
    "                    if type_data_key == \"imgs\":\n",
    "                        if key == \"infos\":\n",
    "                            continue \n",
    "                        if key == \"co_attention_mask\":\n",
    "                            type_data_value[key].append(torch.zeros((data[0][\"imgs\"][\"feat\"].shape[0],self.max_length)))\n",
    "                        if key == \"image_mask\":\n",
    "                            cat =  value[0]\n",
    "                            for i in range(1,len(value)):\n",
    "                                cat = torch.cat((cat, value[i]), dim=0)\n",
    "                            type_data_value[key] = cat\n",
    "                        else:\n",
    "                            type_data_value[key] = torch.cat(value, dim=0)\n",
    "                    else: \n",
    "                        type_data_value[key] = torch.tensor(value)\n",
    "                        \n",
    "        return (data[0][\"imgs\"][\"feat\"], data[0][\"imgs\"][\"pos_enco\"], data[0][\"imgs\"][\"spatial\"],\n",
    "                data[0][\"imgs\"][\"image_mask\"], data[0][\"desc\"][\"tokenized_text\"], data[0][\"desc\"][\"modified_token\"],data[0][\"desc\"][\"masked_lm_token\"],\n",
    "                data[0][\"desc\"][\"input_mask\"], data[0][\"desc\"][\"segment_ids\"], data[0][\"imgs\"][\"co_attention_mask\"], data[0][\"imgs\"][\"infos\"], data[0][\"imgs\"][\"masked_img_labels\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text...\n",
      "  Extracting features...\n",
      "    Action indx 0\n",
      "We have 4 frames\n",
      "slice from 3 -> 4 with 1\n",
      "slice from 2 -> 3 with 1\n",
      "slice from 0 -> 2 with 2\n",
      "> \u001b[0;32m<ipython-input-25-879b1b59bbdf>\u001b[0m(44)\u001b[0;36mflatten_to_one_img\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     42 \u001b[0;31m        \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     43 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 44 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositional_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     45 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     46 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> exit()\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f50bd3341607>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfrcnn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfasterrcnn_resnet50_fpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"short_json_data.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrcnn_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_masked_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-879b1b59bbdf>\u001b[0m in \u001b[0;36mget_data_masked_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m         the masking in the text and image\"\"\"\n\u001b[1;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-879b1b59bbdf>\u001b[0m in \u001b[0;36mimg_tokenizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mimg_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;34m\"\"\"Add the spacial token IMG before each set of features extracted from an image\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_action_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_action_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"imgs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"feat\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-879b1b59bbdf>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# Flatten to one img so that, we have r_0..r_k*nboxes as if it was one image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# To be applied for the 3 lists above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositional_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_to_one_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositional_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-879b1b59bbdf>\u001b[0m in \u001b[0;36mflatten_to_one_img\u001b[0;34m(self, features, positional_encoding, infos)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositional_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-879b1b59bbdf>\u001b[0m in \u001b[0;36mflatten_to_one_img\u001b[0;34m(self, features, positional_encoding, infos)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositional_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "frcnn_model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "data_loader = DataLoader(\"short_json_data.json\", frcnn_model)\n",
    "data = data_loader.get_data_masked_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_masked, pos_enc, spatial, image_mask, tokenized_text, masked_text, masked_lm_token, input_mask, segment_ids, co_attention_mask, infos, masked_img_labels  = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33])\n",
      "torch.Size([37])\n",
      "torch.Size([33, 7])\n",
      "torch.Size([33, 6144])\n"
     ]
    }
   ],
   "source": [
    "print(masked_img_labels.shape)\n",
    "print(tokenized_text.shape)\n",
    "print(pos_enc.shape)\n",
    "print(features_masked.shape)\n",
    "# print(masked_lm_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33])\n"
     ]
    }
   ],
   "source": [
    "image_target = []\n",
    "for i in range(len(infos)):\n",
    "    image_target.append(infos[i][\"objects\"])\n",
    "\n",
    "image_target = torch.cat(image_target, dim=0)\n",
    "print(image_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                    lr=args.learning_rate,\n",
    "                    eps=args.adam_epsilon,\n",
    "                    betas=(0.9, 0.98),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict(model.named_parameters()).items():\n",
    "    if not value.requires_grad:\n",
    "        print(\"This parameter does have grad\", key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (33) must match the size of tensor b (37) at non-singleton dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d199f383b275>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                             \u001b[0mimage_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                             \u001b[0mnext_sentence_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                             output_all_attention_masks=True)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/lab/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/VLN_Bert/vilbert/vilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, image_feat, image_loc, image_pos_input, token_type_ids, attention_mask, image_attention_mask, co_attention_mask, masked_lm_labels, image_label, image_target, next_sentence_label, output_all_attention_masks)\u001b[0m\n\u001b[1;32m   1527\u001b[0m             \u001b[0mimage_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1528\u001b[0m             \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m             output_all_attention_masks=output_all_attention_masks)\n\u001b[0m\u001b[1;32m   1530\u001b[0m         prediction_t, prediction_v, _ = self.action_cls(\n\u001b[1;32m   1531\u001b[0m             sequence_output_t, sequence_output_v, pooled_output_t, pooled_output_v)\n",
      "\u001b[0;32m~/lab/lab/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/VLN_Bert/vilbert/vilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_txt, input_imgs, image_loc, token_type_ids, attention_mask, image_attention_mask, co_attention_mask, task_ids, output_all_encoded_layers, output_all_attention_masks)\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0mextended_co_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m             \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m             \u001b[0moutput_all_attention_masks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_attention_masks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m         )\n\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/lab/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/VLN_Bert/vilbert/vilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, txt_embedding, image_embedding, txt_attention_mask, txt_attention_mask2, image_attention_mask, co_attention_mask, output_all_encoded_layers, output_all_attention_masks)\u001b[0m\n\u001b[1;32m   1062\u001b[0m                     \u001b[0mtxt_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m                     \u001b[0mco_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m                     \u001b[0muse_co_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m                 )\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/lab/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/VLN_Bert/vilbert/vilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor1, attention_mask1, input_tensor2, attention_mask2, co_attention_mask, use_co_attention_mask)\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0mattention_mask2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             \u001b[0mco_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m             \u001b[0muse_co_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m         )\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/lab/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/VLN_Bert/vilbert/vilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor1, attention_mask1, input_tensor2, attention_mask2, co_attention_mask, use_co_attention_mask)\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mattention_scores1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0mattention_scores1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m         \u001b[0mattention_scores1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattention_mask1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m         \u001b[0;31m# if use_co_attention_mask:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;31m# attention_scores1 = attention_scores1 + co_attention_mask.permute(0,1,3,2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (33) must match the size of tensor b (37) at non-singleton dimension 4"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    pred_t, pred_v, att = model(input_ids = tokenized_text.unsqueeze(0).cpu(),\n",
    "                            image_feat = features_masked.unsqueeze(0).cpu(), # Linear(2048*config.max_temporal_memory_buffer, 2048)\n",
    "                            image_loc = spatial.unsqueeze(0).cpu(),  #Linear(in_features=5, out_features=1024, bias=True)\n",
    "                            image_pos_input = pos_enc.unsqueeze(0).cpu(),   #Linear(7, 2048)/(6, 2048)\n",
    "                            token_type_ids = segment_ids.unsqueeze(0).cpu(), \n",
    "                            attention_mask = input_mask.unsqueeze(0).cpu(), \n",
    "                            image_attention_mask = image_mask.unsqueeze(0).cpu(), \n",
    "                            co_attention_mask = co_attention_mask.unsqueeze(0).cpu(),\n",
    "                            masked_lm_labels = masked_lm_token.unsqueeze(0).cpu(), \n",
    "                            image_label = None,\n",
    "                            image_target = None,\n",
    "                            next_sentence_label=None,\n",
    "                            output_all_attention_masks=True)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    masked_lm_loss = model.lang_criterion(pred_t.view(-1, 30522), masked_lm_token.cpu().view(-1))\n",
    "    img_loss = model.vis_criterion(pred_v.view(-1, 91), image_target.cpu()) # why dim 2 (to check) \n",
    "    loss = masked_lm_loss + img_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_masked_lang = model.lang_criterion(pred_t, labels_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([None, None, None, None, None, None, None, None, None, None, None, None], [None, None, None, None, None, None], [None, None, None, None, None, None])\n"
     ]
    }
   ],
   "source": [
    "print((att))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.stack(feature_list, dim=0).float().cuda()\n",
    "    spatials = torch.stack(image_location_list, dim=0).float().cuda()\n",
    "    image_mask = torch.stack(image_mask_list, dim=0).byte().cuda()\n",
    "    co_attention_mask = torch.zeros((num_image, num_boxes, max_length)).cuda()\n",
    "\n",
    "    prediction(text, features, spatials, segment_ids, input_mask, image_mask, co_attention_mask, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[1][\"imgs\"][\"feat\"].shape)\n",
    "concate = torch.cat((data[1][\"imgs\"][\"feat\"].unsqueeze(0), data[0][\"imgs\"][\"feat\"].unsqueeze(0)), dim=0)\n",
    "print(concate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Structure of data: \")\n",
    "print(\"  Number of instructions: \", len(data))\n",
    "print(\"   Per instruction we have: \", data[0].keys())\n",
    "print(\"     The 'desc' of the instruction has: \", data[0][\"desc\"].keys())\n",
    "print(\"       - lists of length tokenized text -->\",len(data[0][\"desc\"][\"tokenized_text\"]))\n",
    "print(\"     The 'imgs' of the instruction has:\", data[0][\"imgs\"].keys())\n",
    "for k, v in data[0][\"imgs\"].items():\n",
    "    print(k, v)\n",
    "    #print(len(data[0][\"imgs\"][\"spatial\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict(model.named_parameters()).items():\n",
    "    if not value.requires_grad:\n",
    "        print(\"This parameter does have grad\", key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda: model = model.cuda(0)\n",
    "#Why do we initialize Tokenizer again?\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.bert_model, do_lower_case=args.do_lower_case\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenize( text):\n",
    "    text = '[CLS]' + text + '[SEP]'\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    return indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"swimming elephant\"\n",
    "indexed_tokens = bert_tokenize(text)\n",
    "#indexed_tokens, modified_indexed_tokens, masked_lm_labels = pretask_mask_lang_tokens(indexed_tokens)\n",
    "\n",
    "query = '[CLS]' + text + '[SEP]'\n",
    "tokens = tokenizer.encode(query)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(question, features, spatials, segment_ids, input_mask, image_mask, co_attention_mask, task_tokens, ):\n",
    "    print('input question size question: ', question.shape)\n",
    "    pos_enc_input = torch.FloatTensor([0,1,2,3,4,5])\n",
    "    print(question.shape)\n",
    "    masked_lm_loss, masked_img_loss, prediction_t, prediction_v, all_attention_mask = model(\n",
    "        input_ids=question, image_feat=features, image_loc=spatials,image_pos_input=pos_enc_input, output_all_attention_masks=True\n",
    "    )\n",
    "    return masked_lm_loss, masked_img_loss, prediction_t, prediction_v, all_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_prediction(query, task, features, infos):\n",
    "\n",
    "#     print(query)\n",
    "#     query = '[CLS]' + query + '[SEP]'\n",
    "#     tokens = tokenizer.encode(query)\n",
    "#     print(tokens)\n",
    "    #tokens= \n",
    "    #tokens = tokenizer.add_special_tokens_single_sentence(tokens)\n",
    "    \n",
    "    tokens = bert_tokenize(query)\n",
    "    segment_ids = [0] * len(tokens)\n",
    "    input_mask = [1] * len(tokens)\n",
    "\n",
    "    max_length = 37\n",
    "    if len(tokens) < max_length:\n",
    "        # Note here we pad in front of the sentence\n",
    "        padding = [0] * (max_length - len(tokens))\n",
    "        tokens = tokens + padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "    text = torch.from_numpy(np.array(tokens)).cuda().unsqueeze(0)\n",
    "    input_mask = torch.from_numpy(np.array(input_mask)).cuda().unsqueeze(0)\n",
    "    segment_ids = torch.from_numpy(np.array(segment_ids)).cuda().unsqueeze(0)\n",
    "    task = torch.from_numpy(np.array(task)).cuda().unsqueeze(0)\n",
    "\n",
    "    num_image = len(infos)\n",
    "\n",
    "    feature_list = []\n",
    "    image_location_list = []\n",
    "    image_mask_list = []\n",
    "    for i in range(num_image):\n",
    "        image_w = infos[i]['image_width']\n",
    "        image_h = infos[i]['image_height']\n",
    "        feature = features[i]\n",
    "        num_boxes = feature.shape[0] #first dim size = number boxes\n",
    "\n",
    "        g_feat = torch.sum(feature, dim=0) / num_boxes # Mean of features of all the selected regions\n",
    "        num_boxes = num_boxes + 1\n",
    "        feature = torch.cat([g_feat.view(1,-1), feature], dim=0)\n",
    "        boxes = infos[i]['bbox']\n",
    "        image_location = np.zeros((boxes.shape[0], 5), dtype=np.float32)\n",
    "        image_location[:,:4] = boxes\n",
    "        image_location[:,4] = (image_location[:,3] - image_location[:,1]) * (image_location[:,2] - image_location[:,0]) / (float(image_w) * float(image_h))\n",
    "        image_location[:,0] = image_location[:,0] / float(image_w)\n",
    "        image_location[:,1] = image_location[:,1] / float(image_h)\n",
    "        image_location[:,2] = image_location[:,2] / float(image_w)\n",
    "        image_location[:,3] = image_location[:,3] / float(image_h)\n",
    "        g_location = np.array([0,0,1,1,1])\n",
    "        image_location = np.concatenate([np.expand_dims(g_location, axis=0), image_location], axis=0)\n",
    "        image_mask = [1] * (int(num_boxes))\n",
    "\n",
    "        feature_list.append(feature)\n",
    "        image_location_list.append(torch.tensor(image_location))\n",
    "        image_mask_list.append(torch.tensor(image_mask))\n",
    "\n",
    "    features = torch.stack(feature_list, dim=0).float().cuda()\n",
    "    spatials = torch.stack(image_location_list, dim=0).float().cuda()\n",
    "    image_mask = torch.stack(image_mask_list, dim=0).byte().cuda()\n",
    "    co_attention_mask = torch.zeros((num_image, num_boxes, max_length)).cuda()\n",
    "#     print(\"text: \", text.shape)\n",
    "#     print(\"feat: \", features.shape)\n",
    "#     print(\"spatials: \", spatials.shape)\n",
    "#     print(\"segments_id: \", segment_ids)\n",
    "#     print(\"input_mask: \", input_mask)\n",
    "#     print(\"image_mask: \", image_mask)\n",
    "#     print(\"coatenttion_mask: \", co_attention_mask)\n",
    "    return prediction(text, features, spatials, segment_ids, input_mask, image_mask, co_attention_mask, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frcnn_model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# read image \n",
    "#pic = \"faster_rcnn/test2.png\"\n",
    "#pic1 = \"faster_rcnn/test.png\"\n",
    "#image_paths = [pic, pic1]\n",
    "\n",
    "\n",
    "\n",
    "image_path = ['demo/1.jpg']\n",
    "features, pos_enc, infos = featureExtractor(image_path, frcnn_model).extract_features()\n",
    "print(\"features: \", type(features))\n",
    "print(\"infos: \", infos)\n",
    "#features, infos = f_extractor.extract_features(image_path, frcnn_model)\n",
    "\n",
    "\n",
    "img = PIL.Image.open(image_path[0]).convert('RGB')\n",
    "img = torch.tensor(np.array(img))\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "    \n",
    "query = \"swimming elephant\"\n",
    "task = [9]\n",
    "masked_lm_loss, masked_img_loss, prediction_t, prediction_v, all_attention_mask = custom_prediction(query, task, features, infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0][\"imgs\"][\"pos_enco\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_lm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " prediction_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "config.visual_target = args.visual_target\n",
    "\n",
    "optimizer_grouped_parameters = []\n",
    "for key, value in dict(model.named_parameters()).items():\n",
    "    if value.requires_grad:\n",
    "        if \"cls\" in key:\n",
    "            lr = args.learning_rate\n",
    "        else:\n",
    "            lr = args.learning_rate * 0.1\n",
    "        if any(nd in key for nd in no_decay): # No decay\n",
    "            optimizer_grouped_parameters += [\n",
    "                {\"params\": [value],\n",
    "                 \"lr\": lr,\n",
    "                 \"weight_decay\": 0.0}\n",
    "            ]\n",
    "        \n",
    "        elif not any(nd in key for nd in no_decay):\n",
    "            optimizer_grouped_parameters += [\n",
    "                {\"params\": [value],\n",
    "                 \"lr\": lr,\n",
    "                 \"weight_decay\": 0.01}\n",
    "            ]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=args.learning_rate,\n",
    "    eps=args.adam_epsilon,\n",
    "    betas=(0.9, 0.98),\n",
    ")\n",
    "\n",
    "num_dataset_points = 19\n",
    "\n",
    "num_train_optimization_steps = int(\n",
    "    num_dataset_points\n",
    "    / args.train_batch_size\n",
    "    / args.gradient_accumulation_steps\n",
    ") * (args.num_train_epochs - args.start_epoch)\n",
    "\n",
    "scheduler = WarmupLinearSchedule(\n",
    "    optimizer,\n",
    "    warmup_steps=args.warmup_proportion * num_train_optimization_steps,\n",
    "    t_total=num_train_optimization_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.cuda()\n",
    "# for state in optimizer.state.values():\n",
    "#     for k, v in state.items():\n",
    "#         if torch.is_tensor(v):\n",
    "#             state[k] = v.cuda()\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"Num examples = %d\", num_dataset_points)\n",
    "logger.info(\"Batch size = %d\", args.train_batch_size)\n",
    "logger.info(\"Num steps = %d\", num_train_optimization_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startIterID = 0\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(int(args.start_epoch), int(args.num_train_epochs)):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataset):\n",
    "        iterId = startIterID + step + (epochId * len(train_dataset))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
